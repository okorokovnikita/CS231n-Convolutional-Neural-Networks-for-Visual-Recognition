{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.390454\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.892407 analytic: 3.051811, relative error: 2.681667e-02\n",
      "numerical: 0.772099 analytic: 0.941960, relative error: 9.909894e-02\n",
      "numerical: -2.961327 analytic: -3.233661, relative error: 4.396039e-02\n",
      "numerical: -2.912538 analytic: -3.014471, relative error: 1.719809e-02\n",
      "numerical: 0.541182 analytic: 0.539011, relative error: 2.009338e-03\n",
      "numerical: -4.253180 analytic: -4.467927, relative error: 2.462376e-02\n",
      "numerical: 2.209896 analytic: 2.524806, relative error: 6.651114e-02\n",
      "numerical: -0.547200 analytic: -0.748348, relative error: 1.552609e-01\n",
      "numerical: -0.598598 analytic: -0.656101, relative error: 4.582956e-02\n",
      "numerical: 3.880569 analytic: 4.164541, relative error: 3.529749e-02\n",
      "numerical: 1.083121 analytic: 1.205303, relative error: 5.339125e-02\n",
      "numerical: 1.037775 analytic: 1.270908, relative error: 1.009806e-01\n",
      "numerical: -4.892288 analytic: -5.375220, relative error: 4.703497e-02\n",
      "numerical: 1.736476 analytic: 1.993352, relative error: 6.887081e-02\n",
      "numerical: 2.599129 analytic: 2.846489, relative error: 4.542353e-02\n",
      "numerical: 0.365252 analytic: 0.406318, relative error: 5.322376e-02\n",
      "numerical: 1.638706 analytic: 1.881125, relative error: 6.887240e-02\n",
      "numerical: -0.267842 analytic: -0.398156, relative error: 1.956663e-01\n",
      "numerical: 2.335908 analytic: 2.574155, relative error: 4.852212e-02\n",
      "numerical: -1.249871 analytic: -1.308647, relative error: 2.297263e-02\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.390454e+00 computed in 0.175780s\n",
      "vectorized loss: 2.390454e+00 computed in 0.007108s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tuning",
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 312.892052\n",
      "iteration 100 / 1500: loss 43.019449\n",
      "iteration 200 / 1500: loss 7.500851\n",
      "iteration 300 / 1500: loss 2.818512\n",
      "iteration 400 / 1500: loss 2.149225\n",
      "iteration 500 / 1500: loss 2.078348\n",
      "iteration 600 / 1500: loss 2.028447\n",
      "iteration 700 / 1500: loss 2.136754\n",
      "iteration 800 / 1500: loss 2.083705\n",
      "iteration 900 / 1500: loss 2.137095\n",
      "iteration 1000 / 1500: loss 2.034424\n",
      "iteration 1100 / 1500: loss 2.163578\n",
      "iteration 1200 / 1500: loss 2.114250\n",
      "iteration 1300 / 1500: loss 2.053082\n",
      "iteration 1400 / 1500: loss 2.000321\n",
      "iteration 0 / 1500: loss 621.691181\n",
      "iteration 100 / 1500: loss 12.865373\n",
      "iteration 200 / 1500: loss 2.269343\n",
      "iteration 300 / 1500: loss 2.092051\n",
      "iteration 400 / 1500: loss 2.147766\n",
      "iteration 500 / 1500: loss 2.042118\n",
      "iteration 600 / 1500: loss 2.113420\n",
      "iteration 700 / 1500: loss 2.139546\n",
      "iteration 800 / 1500: loss 2.122430\n",
      "iteration 900 / 1500: loss 2.092609\n",
      "iteration 1000 / 1500: loss 2.027169\n",
      "iteration 1100 / 1500: loss 2.165695\n",
      "iteration 1200 / 1500: loss 2.083380\n",
      "iteration 1300 / 1500: loss 2.130303\n",
      "iteration 1400 / 1500: loss 2.150400\n",
      "iteration 0 / 1500: loss 768.730834\n",
      "iteration 100 / 1500: loss 6.914755\n",
      "iteration 200 / 1500: loss 2.158219\n",
      "iteration 300 / 1500: loss 2.107412\n",
      "iteration 400 / 1500: loss 2.148360\n",
      "iteration 500 / 1500: loss 2.086701\n",
      "iteration 600 / 1500: loss 2.252928\n",
      "iteration 700 / 1500: loss 2.102413\n",
      "iteration 800 / 1500: loss 2.144477\n",
      "iteration 900 / 1500: loss 2.217501\n",
      "iteration 1000 / 1500: loss 2.130076\n",
      "iteration 1100 / 1500: loss 2.127410\n",
      "iteration 1200 / 1500: loss 2.168059\n",
      "iteration 1300 / 1500: loss 2.148839\n",
      "iteration 1400 / 1500: loss 2.144784\n",
      "iteration 0 / 1500: loss 924.208048\n",
      "iteration 100 / 1500: loss 4.214699\n",
      "iteration 200 / 1500: loss 2.125995\n",
      "iteration 300 / 1500: loss 2.135886\n",
      "iteration 400 / 1500: loss 2.188164\n",
      "iteration 500 / 1500: loss 2.071304\n",
      "iteration 600 / 1500: loss 2.113152\n",
      "iteration 700 / 1500: loss 2.147448\n",
      "iteration 800 / 1500: loss 2.101783\n",
      "iteration 900 / 1500: loss 2.087653\n",
      "iteration 1000 / 1500: loss 2.077756\n",
      "iteration 1100 / 1500: loss 2.152060\n",
      "iteration 1200 / 1500: loss 2.181637\n",
      "iteration 1300 / 1500: loss 2.182290\n",
      "iteration 1400 / 1500: loss 2.111425\n",
      "iteration 0 / 1500: loss 1518.389619\n",
      "iteration 100 / 1500: loss 2.205191\n",
      "iteration 200 / 1500: loss 2.145360\n",
      "iteration 300 / 1500: loss 2.185939\n",
      "iteration 400 / 1500: loss 2.158068\n",
      "iteration 500 / 1500: loss 2.161406\n",
      "iteration 600 / 1500: loss 2.192771\n",
      "iteration 700 / 1500: loss 2.157138\n",
      "iteration 800 / 1500: loss 2.178147\n",
      "iteration 900 / 1500: loss 2.163065\n",
      "iteration 1000 / 1500: loss 2.162711\n",
      "iteration 1100 / 1500: loss 2.197064\n",
      "iteration 1200 / 1500: loss 2.176823\n",
      "iteration 1300 / 1500: loss 2.185724\n",
      "iteration 1400 / 1500: loss 2.150196\n",
      "iteration 0 / 1500: loss 313.918672\n",
      "iteration 100 / 1500: loss 94.805748\n",
      "iteration 200 / 1500: loss 29.631570\n",
      "iteration 300 / 1500: loss 10.366896\n",
      "iteration 400 / 1500: loss 4.481316\n",
      "iteration 500 / 1500: loss 2.768359\n",
      "iteration 600 / 1500: loss 2.273723\n",
      "iteration 700 / 1500: loss 2.157937\n",
      "iteration 800 / 1500: loss 2.084675\n",
      "iteration 900 / 1500: loss 2.091372\n",
      "iteration 1000 / 1500: loss 2.170058\n",
      "iteration 1100 / 1500: loss 2.070524\n",
      "iteration 1200 / 1500: loss 2.076593\n",
      "iteration 1300 / 1500: loss 2.105653\n",
      "iteration 1400 / 1500: loss 2.152449\n",
      "iteration 0 / 1500: loss 624.745680\n",
      "iteration 100 / 1500: loss 57.262297\n",
      "iteration 200 / 1500: loss 6.969278\n",
      "iteration 300 / 1500: loss 2.560224\n",
      "iteration 400 / 1500: loss 2.112256\n",
      "iteration 500 / 1500: loss 2.104145\n",
      "iteration 600 / 1500: loss 2.048557\n",
      "iteration 700 / 1500: loss 2.148482\n",
      "iteration 800 / 1500: loss 2.024370\n",
      "iteration 900 / 1500: loss 2.166080\n",
      "iteration 1000 / 1500: loss 2.102033\n",
      "iteration 1100 / 1500: loss 2.118359\n",
      "iteration 1200 / 1500: loss 2.112124\n",
      "iteration 1300 / 1500: loss 2.106515\n",
      "iteration 1400 / 1500: loss 2.014644\n",
      "iteration 0 / 1500: loss 765.075258\n",
      "iteration 100 / 1500: loss 38.941299\n",
      "iteration 200 / 1500: loss 3.900018\n",
      "iteration 300 / 1500: loss 2.156241\n",
      "iteration 400 / 1500: loss 2.155489\n",
      "iteration 500 / 1500: loss 2.057752\n",
      "iteration 600 / 1500: loss 2.143008\n",
      "iteration 700 / 1500: loss 2.105627\n",
      "iteration 800 / 1500: loss 2.151791\n",
      "iteration 900 / 1500: loss 2.073629\n",
      "iteration 1000 / 1500: loss 2.106145\n",
      "iteration 1100 / 1500: loss 2.127364\n",
      "iteration 1200 / 1500: loss 2.129083\n",
      "iteration 1300 / 1500: loss 2.158284\n",
      "iteration 1400 / 1500: loss 2.108431\n",
      "iteration 0 / 1500: loss 926.430346\n",
      "iteration 100 / 1500: loss 26.437822\n",
      "iteration 200 / 1500: loss 2.731723\n",
      "iteration 300 / 1500: loss 2.211467\n",
      "iteration 400 / 1500: loss 2.174385\n",
      "iteration 500 / 1500: loss 2.106679\n",
      "iteration 600 / 1500: loss 2.142197\n",
      "iteration 700 / 1500: loss 2.155872\n",
      "iteration 800 / 1500: loss 2.098545\n",
      "iteration 900 / 1500: loss 2.174615\n",
      "iteration 1000 / 1500: loss 2.060841\n",
      "iteration 1100 / 1500: loss 2.211988\n",
      "iteration 1200 / 1500: loss 2.160058\n",
      "iteration 1300 / 1500: loss 2.147200\n",
      "iteration 1400 / 1500: loss 2.098882\n",
      "iteration 0 / 1500: loss 1523.560960\n",
      "iteration 100 / 1500: loss 5.585978\n",
      "iteration 200 / 1500: loss 2.192973\n",
      "iteration 300 / 1500: loss 2.198506\n",
      "iteration 400 / 1500: loss 2.132097\n",
      "iteration 500 / 1500: loss 2.184005\n",
      "iteration 600 / 1500: loss 2.155101\n",
      "iteration 700 / 1500: loss 2.175748\n",
      "iteration 800 / 1500: loss 2.167833\n",
      "iteration 900 / 1500: loss 2.186030\n",
      "iteration 1000 / 1500: loss 2.158098\n",
      "iteration 1100 / 1500: loss 2.147550\n",
      "iteration 1200 / 1500: loss 2.116065\n",
      "iteration 1300 / 1500: loss 2.117322\n",
      "iteration 1400 / 1500: loss 2.125345\n",
      "iteration 0 / 1500: loss 312.497500\n",
      "iteration 100 / 1500: loss 139.401245\n",
      "iteration 200 / 1500: loss 63.463467\n",
      "iteration 300 / 1500: loss 29.432361\n",
      "iteration 400 / 1500: loss 14.408934\n",
      "iteration 500 / 1500: loss 7.601804\n",
      "iteration 600 / 1500: loss 4.611746\n",
      "iteration 700 / 1500: loss 3.288886\n",
      "iteration 800 / 1500: loss 2.642974\n",
      "iteration 900 / 1500: loss 2.260458\n",
      "iteration 1000 / 1500: loss 2.119257\n",
      "iteration 1100 / 1500: loss 2.149209\n",
      "iteration 1200 / 1500: loss 2.110643\n",
      "iteration 1300 / 1500: loss 2.039205\n",
      "iteration 1400 / 1500: loss 2.107567\n",
      "iteration 0 / 1500: loss 620.775408\n",
      "iteration 100 / 1500: loss 125.090063\n",
      "iteration 200 / 1500: loss 26.666524\n",
      "iteration 300 / 1500: loss 6.969941\n",
      "iteration 400 / 1500: loss 2.992380\n",
      "iteration 500 / 1500: loss 2.328941\n",
      "iteration 600 / 1500: loss 2.040755\n",
      "iteration 700 / 1500: loss 2.104666\n",
      "iteration 800 / 1500: loss 2.149642\n",
      "iteration 900 / 1500: loss 2.114481\n",
      "iteration 1000 / 1500: loss 2.149623\n",
      "iteration 1100 / 1500: loss 2.078865\n",
      "iteration 1200 / 1500: loss 2.071709\n",
      "iteration 1300 / 1500: loss 2.162382\n",
      "iteration 1400 / 1500: loss 2.098818\n",
      "iteration 0 / 1500: loss 773.916072\n",
      "iteration 100 / 1500: loss 104.965148\n",
      "iteration 200 / 1500: loss 15.850962\n",
      "iteration 300 / 1500: loss 4.006981\n",
      "iteration 400 / 1500: loss 2.347293\n",
      "iteration 500 / 1500: loss 2.114462\n",
      "iteration 600 / 1500: loss 2.132448\n",
      "iteration 700 / 1500: loss 2.082406\n",
      "iteration 800 / 1500: loss 2.097426\n",
      "iteration 900 / 1500: loss 2.061642\n",
      "iteration 1000 / 1500: loss 2.098041\n",
      "iteration 1100 / 1500: loss 2.074939\n",
      "iteration 1200 / 1500: loss 2.160543\n",
      "iteration 1300 / 1500: loss 2.050586\n",
      "iteration 1400 / 1500: loss 2.043123\n",
      "iteration 0 / 1500: loss 924.776861\n",
      "iteration 100 / 1500: loss 84.213193\n",
      "iteration 200 / 1500: loss 9.432585\n",
      "iteration 300 / 1500: loss 2.706855\n",
      "iteration 400 / 1500: loss 2.095774\n",
      "iteration 500 / 1500: loss 2.112138\n",
      "iteration 600 / 1500: loss 2.090477\n",
      "iteration 700 / 1500: loss 2.115206\n",
      "iteration 800 / 1500: loss 2.186206\n",
      "iteration 900 / 1500: loss 2.121671\n",
      "iteration 1000 / 1500: loss 2.060287\n",
      "iteration 1100 / 1500: loss 2.118565\n",
      "iteration 1200 / 1500: loss 2.107200\n",
      "iteration 1300 / 1500: loss 2.166971\n",
      "iteration 1400 / 1500: loss 2.070969\n",
      "iteration 0 / 1500: loss 1528.847496\n",
      "iteration 100 / 1500: loss 28.802821\n",
      "iteration 200 / 1500: loss 2.644975\n",
      "iteration 300 / 1500: loss 2.166999\n",
      "iteration 400 / 1500: loss 2.166148\n",
      "iteration 500 / 1500: loss 2.117224\n",
      "iteration 600 / 1500: loss 2.147292\n",
      "iteration 700 / 1500: loss 2.139596\n",
      "iteration 800 / 1500: loss 2.214111\n",
      "iteration 900 / 1500: loss 2.153288\n",
      "iteration 1000 / 1500: loss 2.212664\n",
      "iteration 1100 / 1500: loss 2.133090\n",
      "iteration 1200 / 1500: loss 2.150186\n",
      "iteration 1300 / 1500: loss 2.100644\n",
      "iteration 1400 / 1500: loss 2.110640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 310.765330\n",
      "iteration 100 / 1500: loss 207.117468\n",
      "iteration 200 / 1500: loss 139.124783\n",
      "iteration 300 / 1500: loss 93.808153\n",
      "iteration 400 / 1500: loss 63.491774\n",
      "iteration 500 / 1500: loss 42.950814\n",
      "iteration 600 / 1500: loss 29.491727\n",
      "iteration 700 / 1500: loss 20.394881\n",
      "iteration 800 / 1500: loss 14.277776\n",
      "iteration 900 / 1500: loss 10.247158\n",
      "iteration 1000 / 1500: loss 7.602555\n",
      "iteration 1100 / 1500: loss 5.867529\n",
      "iteration 1200 / 1500: loss 4.510408\n",
      "iteration 1300 / 1500: loss 3.627141\n",
      "iteration 1400 / 1500: loss 3.316331\n",
      "iteration 0 / 1500: loss 626.645684\n",
      "iteration 100 / 1500: loss 280.948676\n",
      "iteration 200 / 1500: loss 126.826653\n",
      "iteration 300 / 1500: loss 57.854308\n",
      "iteration 400 / 1500: loss 27.110580\n",
      "iteration 500 / 1500: loss 13.334326\n",
      "iteration 600 / 1500: loss 7.093504\n",
      "iteration 700 / 1500: loss 4.388699\n",
      "iteration 800 / 1500: loss 3.091857\n",
      "iteration 900 / 1500: loss 2.649202\n",
      "iteration 1000 / 1500: loss 2.291196\n",
      "iteration 1100 / 1500: loss 2.243641\n",
      "iteration 1200 / 1500: loss 2.210107\n",
      "iteration 1300 / 1500: loss 2.082799\n",
      "iteration 1400 / 1500: loss 2.017321\n",
      "iteration 0 / 1500: loss 779.654601\n",
      "iteration 100 / 1500: loss 286.224071\n",
      "iteration 200 / 1500: loss 105.988967\n",
      "iteration 300 / 1500: loss 40.127335\n",
      "iteration 400 / 1500: loss 16.128508\n",
      "iteration 500 / 1500: loss 7.158293\n",
      "iteration 600 / 1500: loss 4.013265\n",
      "iteration 700 / 1500: loss 2.854281\n",
      "iteration 800 / 1500: loss 2.334195\n",
      "iteration 900 / 1500: loss 2.217488\n",
      "iteration 1000 / 1500: loss 2.166963\n",
      "iteration 1100 / 1500: loss 2.122453\n",
      "iteration 1200 / 1500: loss 2.125469\n",
      "iteration 1300 / 1500: loss 2.163808\n",
      "iteration 1400 / 1500: loss 2.037765\n",
      "iteration 0 / 1500: loss 918.112988\n",
      "iteration 100 / 1500: loss 275.761098\n",
      "iteration 200 / 1500: loss 84.135530\n",
      "iteration 300 / 1500: loss 26.660058\n",
      "iteration 400 / 1500: loss 9.402723\n",
      "iteration 500 / 1500: loss 4.313053\n",
      "iteration 600 / 1500: loss 2.740706\n",
      "iteration 700 / 1500: loss 2.352626\n",
      "iteration 800 / 1500: loss 2.180367\n",
      "iteration 900 / 1500: loss 2.110087\n",
      "iteration 1000 / 1500: loss 2.203881\n",
      "iteration 1100 / 1500: loss 2.178444\n",
      "iteration 1200 / 1500: loss 2.121590\n",
      "iteration 1300 / 1500: loss 2.096102\n",
      "iteration 1400 / 1500: loss 2.138938\n",
      "iteration 0 / 1500: loss 1536.092514\n",
      "iteration 100 / 1500: loss 206.978958\n",
      "iteration 200 / 1500: loss 29.508021\n",
      "iteration 300 / 1500: loss 5.803414\n",
      "iteration 400 / 1500: loss 2.652780\n",
      "iteration 500 / 1500: loss 2.241409\n",
      "iteration 600 / 1500: loss 2.167025\n",
      "iteration 700 / 1500: loss 2.153407\n",
      "iteration 800 / 1500: loss 2.115427\n",
      "iteration 900 / 1500: loss 2.140208\n",
      "iteration 1000 / 1500: loss 2.147248\n",
      "iteration 1100 / 1500: loss 2.094906\n",
      "iteration 1200 / 1500: loss 2.162910\n",
      "iteration 1300 / 1500: loss 2.105327\n",
      "iteration 1400 / 1500: loss 2.172419\n",
      "iteration 0 / 1500: loss 310.948114\n",
      "iteration 100 / 1500: loss 254.029874\n",
      "iteration 200 / 1500: loss 207.977727\n",
      "iteration 300 / 1500: loss 170.464699\n",
      "iteration 400 / 1500: loss 139.540503\n",
      "iteration 500 / 1500: loss 114.649224\n",
      "iteration 600 / 1500: loss 94.198586\n",
      "iteration 700 / 1500: loss 77.251236\n",
      "iteration 800 / 1500: loss 63.504507\n",
      "iteration 900 / 1500: loss 52.399516\n",
      "iteration 1000 / 1500: loss 43.252702\n",
      "iteration 1100 / 1500: loss 35.586668\n",
      "iteration 1200 / 1500: loss 29.618896\n",
      "iteration 1300 / 1500: loss 24.537971\n",
      "iteration 1400 / 1500: loss 20.563233\n",
      "iteration 0 / 1500: loss 632.622307\n",
      "iteration 100 / 1500: loss 423.319081\n",
      "iteration 200 / 1500: loss 283.847434\n",
      "iteration 300 / 1500: loss 190.581595\n",
      "iteration 400 / 1500: loss 128.213093\n",
      "iteration 500 / 1500: loss 86.529135\n",
      "iteration 600 / 1500: loss 58.641979\n",
      "iteration 700 / 1500: loss 39.889944\n",
      "iteration 800 / 1500: loss 27.327344\n",
      "iteration 900 / 1500: loss 18.999432\n",
      "iteration 1000 / 1500: loss 13.393375\n",
      "iteration 1100 / 1500: loss 9.622714\n",
      "iteration 1200 / 1500: loss 7.207174\n",
      "iteration 1300 / 1500: loss 5.484349\n",
      "iteration 1400 / 1500: loss 4.343989\n",
      "iteration 0 / 1500: loss 772.860465\n",
      "iteration 100 / 1500: loss 468.061143\n",
      "iteration 200 / 1500: loss 283.872092\n",
      "iteration 300 / 1500: loss 172.836012\n",
      "iteration 400 / 1500: loss 105.428118\n",
      "iteration 500 / 1500: loss 64.798235\n",
      "iteration 600 / 1500: loss 39.984724\n",
      "iteration 700 / 1500: loss 25.065932\n",
      "iteration 800 / 1500: loss 15.975465\n",
      "iteration 900 / 1500: loss 10.581475\n",
      "iteration 1000 / 1500: loss 7.148575\n",
      "iteration 1100 / 1500: loss 5.269253\n",
      "iteration 1200 / 1500: loss 4.001582\n",
      "iteration 1300 / 1500: loss 3.285706\n",
      "iteration 1400 / 1500: loss 2.814535\n",
      "iteration 0 / 1500: loss 940.105162\n",
      "iteration 100 / 1500: loss 515.398229\n",
      "iteration 200 / 1500: loss 283.227008\n",
      "iteration 300 / 1500: loss 156.039336\n",
      "iteration 400 / 1500: loss 86.488287\n",
      "iteration 500 / 1500: loss 48.390672\n",
      "iteration 600 / 1500: loss 27.381542\n",
      "iteration 700 / 1500: loss 15.936804\n",
      "iteration 800 / 1500: loss 9.717375\n",
      "iteration 900 / 1500: loss 6.381477\n",
      "iteration 1000 / 1500: loss 4.405179\n",
      "iteration 1100 / 1500: loss 3.306534\n",
      "iteration 1200 / 1500: loss 2.800015\n",
      "iteration 1300 / 1500: loss 2.506195\n",
      "iteration 1400 / 1500: loss 2.376518\n",
      "iteration 0 / 1500: loss 1542.505267\n",
      "iteration 100 / 1500: loss 566.371238\n",
      "iteration 200 / 1500: loss 208.895264\n",
      "iteration 300 / 1500: loss 77.993910\n",
      "iteration 400 / 1500: loss 29.906280\n",
      "iteration 500 / 1500: loss 12.301909\n",
      "iteration 600 / 1500: loss 5.837712\n",
      "iteration 700 / 1500: loss 3.472484\n",
      "iteration 800 / 1500: loss 2.765780\n",
      "iteration 900 / 1500: loss 2.332108\n",
      "iteration 1000 / 1500: loss 2.193214\n",
      "iteration 1100 / 1500: loss 2.156092\n",
      "iteration 1200 / 1500: loss 2.185908\n",
      "iteration 1300 / 1500: loss 2.165131\n",
      "iteration 1400 / 1500: loss 2.164804\n",
      "iteration 0 / 1500: loss 311.229717\n",
      "iteration 100 / 1500: loss 69.886382\n",
      "iteration 200 / 1500: loss 67.398553\n",
      "iteration 300 / 1500: loss 59.736607\n",
      "iteration 400 / 1500: loss 63.404752\n",
      "iteration 500 / 1500: loss 86.673741\n",
      "iteration 600 / 1500: loss 58.635018\n",
      "iteration 700 / 1500: loss 64.061985\n",
      "iteration 800 / 1500: loss 50.211152\n",
      "iteration 900 / 1500: loss 66.123887\n",
      "iteration 1000 / 1500: loss 60.272080\n",
      "iteration 1100 / 1500: loss 71.998198\n",
      "iteration 1200 / 1500: loss 75.879426\n",
      "iteration 1300 / 1500: loss 89.236814\n",
      "iteration 1400 / 1500: loss 74.053353\n",
      "iteration 0 / 1500: loss 620.341914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaokorokov/Documents/Education/Online courses/CS231n Convolutional Neural Networks for Visual Recognition/assignment1/cs231n/classifiers/softmax.py:88: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.sum(-np.log(yi_scores))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss inf\n",
      "iteration 400 / 1500: loss inf\n",
      "iteration 500 / 1500: loss inf\n",
      "iteration 600 / 1500: loss inf\n",
      "iteration 700 / 1500: loss inf\n",
      "iteration 800 / 1500: loss inf\n",
      "iteration 900 / 1500: loss inf\n",
      "iteration 1000 / 1500: loss inf\n",
      "iteration 1100 / 1500: loss inf\n",
      "iteration 1200 / 1500: loss inf\n",
      "iteration 1300 / 1500: loss inf\n",
      "iteration 1400 / 1500: loss inf\n",
      "iteration 0 / 1500: loss 769.638856\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss inf\n",
      "iteration 400 / 1500: loss inf\n",
      "iteration 500 / 1500: loss inf\n",
      "iteration 600 / 1500: loss inf\n",
      "iteration 700 / 1500: loss inf\n",
      "iteration 800 / 1500: loss inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaokorokov/Documents/Education/Online courses/CS231n Convolutional Neural Networks for Visual Recognition/assignment1/cs231n/classifiers/softmax.py:96: RuntimeWarning: overflow encountered in double_scalars\n",
      "  loss += reg * np.sum(W * W)\n",
      "/Users/nikitaokorokov/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/nikitaokorokov/Documents/Education/Online courses/CS231n Convolutional Neural Networks for Visual Recognition/assignment1/cs231n/classifiers/softmax.py:96: RuntimeWarning: overflow encountered in multiply\n",
      "  loss += reg * np.sum(W * W)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1500: loss inf\n",
      "iteration 1000 / 1500: loss inf\n",
      "iteration 1100 / 1500: loss inf\n",
      "iteration 1200 / 1500: loss inf\n",
      "iteration 1300 / 1500: loss inf\n",
      "iteration 1400 / 1500: loss inf\n",
      "iteration 0 / 1500: loss 932.026812\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss inf\n",
      "iteration 400 / 1500: loss inf\n",
      "iteration 500 / 1500: loss inf\n",
      "iteration 600 / 1500: loss inf\n",
      "iteration 700 / 1500: loss inf\n",
      "iteration 800 / 1500: loss inf\n",
      "iteration 900 / 1500: loss inf\n",
      "iteration 1000 / 1500: loss inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaokorokov/Documents/Education/Online courses/CS231n Convolutional Neural Networks for Visual Recognition/assignment1/cs231n/classifiers/softmax.py:83: RuntimeWarning: overflow encountered in subtract\n",
      "  scores = scores - np.max(scores, axis = 1, keepdims = True)\n",
      "/Users/nikitaokorokov/Documents/Education/Online courses/CS231n Convolutional Neural Networks for Visual Recognition/assignment1/cs231n/classifiers/softmax.py:97: RuntimeWarning: overflow encountered in multiply\n",
      "  dW += 2*reg*W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1527.031140\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss inf\n",
      "iteration 400 / 1500: loss inf\n",
      "iteration 500 / 1500: loss inf\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 313.716726\n",
      "iteration 100 / 1500: loss 25.987261\n",
      "iteration 200 / 1500: loss 27.137918\n",
      "iteration 300 / 1500: loss 23.990264\n",
      "iteration 400 / 1500: loss 18.935660\n",
      "iteration 500 / 1500: loss 24.324875\n",
      "iteration 600 / 1500: loss 16.231685\n",
      "iteration 700 / 1500: loss 20.197294\n",
      "iteration 800 / 1500: loss 25.104758\n",
      "iteration 900 / 1500: loss 21.056445\n",
      "iteration 1000 / 1500: loss 28.616752\n",
      "iteration 1100 / 1500: loss 19.136485\n",
      "iteration 1200 / 1500: loss 15.969130\n",
      "iteration 1300 / 1500: loss 18.505036\n",
      "iteration 1400 / 1500: loss 20.428253\n",
      "iteration 0 / 1500: loss 622.326018\n",
      "iteration 100 / 1500: loss 66.130678\n",
      "iteration 200 / 1500: loss 68.693932\n",
      "iteration 300 / 1500: loss 58.712990\n",
      "iteration 400 / 1500: loss 62.785294\n",
      "iteration 500 / 1500: loss 72.002111\n",
      "iteration 600 / 1500: loss 55.557541\n",
      "iteration 700 / 1500: loss 61.536471\n",
      "iteration 800 / 1500: loss 69.734723\n",
      "iteration 900 / 1500: loss 70.265977\n",
      "iteration 1000 / 1500: loss 66.596683\n",
      "iteration 1100 / 1500: loss 58.089278\n",
      "iteration 1200 / 1500: loss 66.580632\n",
      "iteration 1300 / 1500: loss 71.290937\n",
      "iteration 1400 / 1500: loss 70.045197\n",
      "iteration 0 / 1500: loss 775.680846\n",
      "iteration 100 / 1500: loss 142.617974\n",
      "iteration 200 / 1500: loss 147.323696\n",
      "iteration 300 / 1500: loss 145.754823\n",
      "iteration 400 / 1500: loss 135.170063\n",
      "iteration 500 / 1500: loss 160.041457\n",
      "iteration 600 / 1500: loss 155.161468\n",
      "iteration 700 / 1500: loss 152.204199\n",
      "iteration 800 / 1500: loss 145.039246\n",
      "iteration 900 / 1500: loss 141.911474\n",
      "iteration 1000 / 1500: loss 139.724684\n",
      "iteration 1100 / 1500: loss 147.026499\n",
      "iteration 1200 / 1500: loss 142.108689\n",
      "iteration 1300 / 1500: loss 136.876164\n",
      "iteration 1400 / 1500: loss 153.295105\n",
      "iteration 0 / 1500: loss 925.009924\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss inf\n",
      "iteration 400 / 1500: loss inf\n",
      "iteration 500 / 1500: loss inf\n",
      "iteration 600 / 1500: loss inf\n",
      "iteration 700 / 1500: loss inf\n",
      "iteration 800 / 1500: loss inf\n",
      "iteration 900 / 1500: loss inf\n",
      "iteration 1000 / 1500: loss inf\n",
      "iteration 1100 / 1500: loss inf\n",
      "iteration 1200 / 1500: loss inf\n",
      "iteration 1300 / 1500: loss inf\n",
      "iteration 1400 / 1500: loss inf\n",
      "iteration 0 / 1500: loss 1530.494780\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss inf\n",
      "iteration 400 / 1500: loss inf\n",
      "iteration 500 / 1500: loss inf\n",
      "iteration 600 / 1500: loss inf\n",
      "iteration 700 / 1500: loss inf\n",
      "iteration 800 / 1500: loss inf\n",
      "iteration 900 / 1500: loss inf\n",
      "iteration 1000 / 1500: loss inf\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 310.305898\n",
      "iteration 100 / 1500: loss 3.526397\n",
      "iteration 200 / 1500: loss 4.072261\n",
      "iteration 300 / 1500: loss 5.387098\n",
      "iteration 400 / 1500: loss 4.622017\n",
      "iteration 500 / 1500: loss 3.206426\n",
      "iteration 600 / 1500: loss 2.866045\n",
      "iteration 700 / 1500: loss 3.963544\n",
      "iteration 800 / 1500: loss 4.075826\n",
      "iteration 900 / 1500: loss 3.101890\n",
      "iteration 1000 / 1500: loss 5.521489\n",
      "iteration 1100 / 1500: loss 3.649668\n",
      "iteration 1200 / 1500: loss 6.654509\n",
      "iteration 1300 / 1500: loss 3.358488\n",
      "iteration 1400 / 1500: loss 5.824978\n",
      "iteration 0 / 1500: loss 621.786000\n",
      "iteration 100 / 1500: loss 6.856752\n",
      "iteration 200 / 1500: loss 5.095545\n",
      "iteration 300 / 1500: loss 5.296110\n",
      "iteration 400 / 1500: loss 5.211408\n",
      "iteration 500 / 1500: loss 4.800926\n",
      "iteration 600 / 1500: loss 4.002020\n",
      "iteration 700 / 1500: loss 4.161312\n",
      "iteration 800 / 1500: loss 6.142692\n",
      "iteration 900 / 1500: loss 5.654496\n",
      "iteration 1000 / 1500: loss 2.984385\n",
      "iteration 1100 / 1500: loss 5.373237\n",
      "iteration 1200 / 1500: loss 3.444015\n",
      "iteration 1300 / 1500: loss 4.389410\n",
      "iteration 1400 / 1500: loss 5.620405\n",
      "iteration 0 / 1500: loss 778.828485\n",
      "iteration 100 / 1500: loss 5.356721\n",
      "iteration 200 / 1500: loss 6.563059\n",
      "iteration 300 / 1500: loss 7.506996\n",
      "iteration 400 / 1500: loss 7.435828\n",
      "iteration 500 / 1500: loss 6.632854\n",
      "iteration 600 / 1500: loss 9.397336\n",
      "iteration 700 / 1500: loss 7.406719\n",
      "iteration 800 / 1500: loss 5.744556\n",
      "iteration 900 / 1500: loss 5.702376\n",
      "iteration 1000 / 1500: loss 4.621088\n",
      "iteration 1100 / 1500: loss 5.990119\n",
      "iteration 1200 / 1500: loss 6.015963\n",
      "iteration 1300 / 1500: loss 7.822993\n",
      "iteration 1400 / 1500: loss 4.826918\n",
      "iteration 0 / 1500: loss 928.602174\n",
      "iteration 100 / 1500: loss 9.835363\n",
      "iteration 200 / 1500: loss 7.807000\n",
      "iteration 300 / 1500: loss 7.441981\n",
      "iteration 400 / 1500: loss 8.438179\n",
      "iteration 500 / 1500: loss 8.502033\n",
      "iteration 600 / 1500: loss 8.686912\n",
      "iteration 700 / 1500: loss 7.402802\n",
      "iteration 800 / 1500: loss 7.543098\n",
      "iteration 900 / 1500: loss 8.476961\n",
      "iteration 1000 / 1500: loss 4.048821\n",
      "iteration 1100 / 1500: loss 4.978714\n",
      "iteration 1200 / 1500: loss 7.182403\n",
      "iteration 1300 / 1500: loss 6.203075\n",
      "iteration 1400 / 1500: loss 9.068298\n",
      "iteration 0 / 1500: loss 1551.169655\n",
      "iteration 100 / 1500: loss 13.305930\n",
      "iteration 200 / 1500: loss 13.314606\n",
      "iteration 300 / 1500: loss 15.422795\n",
      "iteration 400 / 1500: loss 17.489465\n",
      "iteration 500 / 1500: loss 15.480082\n",
      "iteration 600 / 1500: loss 17.053692\n",
      "iteration 700 / 1500: loss 16.603139\n",
      "iteration 800 / 1500: loss 16.105912\n",
      "iteration 900 / 1500: loss 15.363857\n",
      "iteration 1000 / 1500: loss 16.277565\n",
      "iteration 1100 / 1500: loss 16.254404\n",
      "iteration 1200 / 1500: loss 14.774817\n",
      "iteration 1300 / 1500: loss 16.401220\n",
      "iteration 1400 / 1500: loss 14.939125\n",
      "iteration 0 / 1500: loss 311.954533\n",
      "iteration 100 / 1500: loss inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaokorokov/Documents/Education/Online courses/CS231n Convolutional Neural Networks for Visual Recognition/assignment1/cs231n/classifiers/softmax.py:83: RuntimeWarning: invalid value encountered in subtract\n",
      "  scores = scores - np.max(scores, axis = 1, keepdims = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 628.036384\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 778.166519\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 913.680020\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1543.463105\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 313.306675\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 623.399093\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 770.554442\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 931.962269\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1565.549368\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 313.362824\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 618.750825\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 770.780295\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 934.314408\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1546.298383\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "lr 5.000000e-08 reg 1.000000e+04 train accuracy: 0.311776 val accuracy: 0.329000\n",
      "lr 5.000000e-08 reg 2.000000e+04 train accuracy: 0.316653 val accuracy: 0.331000\n",
      "lr 5.000000e-08 reg 2.500000e+04 train accuracy: 0.322571 val accuracy: 0.329000\n",
      "lr 5.000000e-08 reg 3.000000e+04 train accuracy: 0.314449 val accuracy: 0.331000\n",
      "lr 5.000000e-08 reg 5.000000e+04 train accuracy: 0.301735 val accuracy: 0.319000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.347429 val accuracy: 0.351000\n",
      "lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.331082 val accuracy: 0.347000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.323837 val accuracy: 0.337000\n",
      "lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.317265 val accuracy: 0.330000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.301837 val accuracy: 0.316000\n",
      "lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.354469 val accuracy: 0.376000\n",
      "lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.323735 val accuracy: 0.337000\n",
      "lr 2.000000e-07 reg 2.500000e+04 train accuracy: 0.323367 val accuracy: 0.337000\n",
      "lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.315918 val accuracy: 0.322000\n",
      "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.295735 val accuracy: 0.308000\n",
      "lr 3.000000e-07 reg 1.000000e+04 train accuracy: 0.344408 val accuracy: 0.358000\n",
      "lr 3.000000e-07 reg 2.000000e+04 train accuracy: 0.320531 val accuracy: 0.331000\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.322755 val accuracy: 0.336000\n",
      "lr 3.000000e-07 reg 3.000000e+04 train accuracy: 0.319898 val accuracy: 0.333000\n",
      "lr 3.000000e-07 reg 5.000000e+04 train accuracy: 0.294898 val accuracy: 0.309000\n",
      "lr 5.000000e-07 reg 1.000000e+04 train accuracy: 0.346918 val accuracy: 0.363000\n",
      "lr 5.000000e-07 reg 2.000000e+04 train accuracy: 0.318735 val accuracy: 0.329000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.319041 val accuracy: 0.330000\n",
      "lr 5.000000e-07 reg 3.000000e+04 train accuracy: 0.309429 val accuracy: 0.322000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.292571 val accuracy: 0.307000\n",
      "lr 1.000000e-05 reg 1.000000e+04 train accuracy: 0.213490 val accuracy: 0.232000\n",
      "lr 1.000000e-05 reg 2.000000e+04 train accuracy: 0.146000 val accuracy: 0.154000\n",
      "lr 1.000000e-05 reg 2.500000e+04 train accuracy: 0.176837 val accuracy: 0.186000\n",
      "lr 1.000000e-05 reg 3.000000e+04 train accuracy: 0.148571 val accuracy: 0.154000\n",
      "lr 1.000000e-05 reg 5.000000e+04 train accuracy: 0.130510 val accuracy: 0.140000\n",
      "lr 3.000000e-05 reg 1.000000e+04 train accuracy: 0.135755 val accuracy: 0.129000\n",
      "lr 3.000000e-05 reg 2.000000e+04 train accuracy: 0.106020 val accuracy: 0.101000\n",
      "lr 3.000000e-05 reg 2.500000e+04 train accuracy: 0.106000 val accuracy: 0.119000\n",
      "lr 3.000000e-05 reg 3.000000e+04 train accuracy: 0.065184 val accuracy: 0.072000\n",
      "lr 3.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 1.000000e+04 train accuracy: 0.099204 val accuracy: 0.106000\n",
      "lr 5.000000e-05 reg 2.000000e+04 train accuracy: 0.068592 val accuracy: 0.065000\n",
      "lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.095592 val accuracy: 0.103000\n",
      "lr 5.000000e-05 reg 3.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 2.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 3.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-03 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-03 reg 2.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-03 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-03 reg 3.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-03 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 2.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 3.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.376000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# Provided as a reference. You may or may not want to change these hyperparameters\n",
    "learning_rates = [5e-7, 3e-7, 2e-7, 1e-7, 0.5e-7, 5e-5, 3e-5, 1e-5, 5e-3, 3e-3, 1e-3]\n",
    "regularization_strengths = [1e4, 2e4, 2.5e4, 3e4, 5e4]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rate=lr, reg=rs, num_iters=1500, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        results[(lr, rs)] = (train_acc, val_acc)\n",
    "        \n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.363000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
